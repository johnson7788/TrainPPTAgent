#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Date  : 2025/6/20 10:02
# @File  : tools.py.py
# @Author: johnson
# @Contact : github: johnson7788
# @Desc  : æœç´¢å›¾ç‰‡ï¼Œç”¨äºPPTçš„é…å›¾

import re
import os
import time
import logging
import httpx
from datetime import datetime
import random
import hashlib
from pathlib import Path
from google.adk.tools import ToolContext
import requests
from urllib.parse import quote
import json
from typing import List, Dict, Any
from .weixin_search import sogou_weixin_search,get_real_url,get_article_content

logger = logging.getLogger(__name__)

async def SearchImage(query: str, count: int = 1, tool_context: ToolContext = None) -> List[Dict[str, Any]]:
    """
    æ ¹æ®å…³é”®è¯æœç´¢å¯¹åº”çš„å›¾ç‰‡ï¼Œä½¿ç”¨Pexels API
    :param query: æœç´¢å…³é”®è¯
    :param count: è¿”å›å›¾ç‰‡æ•°é‡ï¼Œé»˜è®¤1å¼ 
    :param tool_context: å·¥å…·ä¸Šä¸‹æ–‡
    :return: å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨
    """
    try:
        # ä»ç¯å¢ƒå˜é‡è·å–Pexels APIå¯†é’¥
        pexels_api_key = os.getenv("PEXELS_API_KEY")
        if not pexels_api_key:
            # å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            return _get_simulate_images(query, count)
        
        # æ„å»ºPexels APIè¯·æ±‚
        headers = {
            "Authorization": pexels_api_key
        }
        
        # å¯¹æŸ¥è¯¢è¯è¿›è¡ŒURLç¼–ç 
        encoded_query = quote(query)
        url = f"https://api.pexels.com/v1/search?query={encoded_query}&per_page={min(count, 80)}&orientation=landscape"
        
        print(f"æ­£åœ¨æœç´¢å›¾ç‰‡ï¼Œå…³é”®è¯: {query}")
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        photos = data.get("photos", [])
        
        if not photos:
            print(f"æœªæ‰¾åˆ°å…³é”®è¯ '{query}' ç›¸å…³çš„å›¾ç‰‡ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return _get_simulate_images(query, count)
        
        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        image_results = []
        for photo in photos[:count]:
            image_info = {
                "id": photo.get("id", random.randint(100000, 999999)),
                "src": photo.get("src", {}).get("large2x", photo.get("src", {}).get("large", "")),
                "width": photo.get("width", 1920),
                "height": photo.get("height", 1080),
                "alt": photo.get("alt", query),
                "photographer": photo.get("photographer", "Unknown"),
                "url": photo.get("url", "")
            }
            image_results.append(image_info)
        
        print(f"æˆåŠŸæœç´¢åˆ° {len(image_results)} å¼ å›¾ç‰‡")
        return image_results
        
    except requests.exceptions.RequestException as e:
        print(f"Pexels APIè¯·æ±‚å¤±è´¥: {e}")
        return _get_simulate_images(query, count)
    except Exception as e:
        print(f"å›¾ç‰‡æœç´¢å‡ºé”™: {e}")
        return _get_simulate_images(query, count)


def _get_simulate_images(query: str, count: int) -> List[Dict[str, Any]]:
    """
    è·å–æ¨¡æ‹Ÿå›¾ç‰‡æ•°æ®ï¼ˆå½“APIä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
    """
    # æ ¹æ®æŸ¥è¯¢è¯é€‰æ‹©ä¸åŒçš„æ¨¡æ‹Ÿå›¾ç‰‡
    query_lower = query.lower()
    
    # é¢„è®¾çš„å›¾ç‰‡æ± ï¼ŒæŒ‰ä¸»é¢˜åˆ†ç±»
    image_pools = {
        "technology": [
            "https://images.pexels.com/photos/3861969/pexels-photo-3861969.jpeg",
            "https://images.pexels.com/photos/3861967/pexels-photo-3861967.jpeg",
            "https://images.pexels.com/photos/3861966/pexels-photo-3861966.jpeg",
            "https://images.pexels.com/photos/3861965/pexels-photo-3861965.jpeg",
            "https://images.pexels.com/photos/3861964/pexels-photo-3861964.jpeg",
        ],
        "business": [
            "https://images.pexels.com/photos/3183150/pexels-photo-3183150.jpeg",
            "https://images.pexels.com/photos/3183153/pexels-photo-3183153.jpeg",
            "https://images.pexels.com/photos/3183154/pexels-photo-3183154.jpeg",
            "https://images.pexels.com/photos/3183155/pexels-photo-3183155.jpeg",
            "https://images.pexels.com/photos/3183156/pexels-photo-3183156.jpeg",
        ],
        "nature": [
            "https://images.pexels.com/photos/3225517/pexels-photo-3225517.jpeg",
            "https://images.pexels.com/photos/3225518/pexels-photo-3225518.jpeg",
            "https://images.pexels.com/photos/3225519/pexels-photo-3225519.jpeg",
            "https://images.pexels.com/photos/3225520/pexels-photo-3225520.jpeg",
            "https://images.pexels.com/photos/3225521/pexels-photo-3225521.jpeg",
        ],
        "abstract": [
            "https://images.pexels.com/photos/3255761/pexels-photo-3255761.jpeg",
            "https://images.pexels.com/photos/3255762/pexels-photo-3255762.jpeg",
            "https://images.pexels.com/photos/3255763/pexels-photo-3255763.jpeg",
            "https://images.pexels.com/photos/3255764/pexels-photo-3255764.jpeg",
            "https://images.pexels.com/photos/3255765/pexels-photo-3255765.jpeg",
        ]
    }
    
    # æ ¹æ®æŸ¥è¯¢è¯é€‰æ‹©æœ€åŒ¹é…çš„å›¾ç‰‡æ± 
    selected_pool = "abstract"  # é»˜è®¤
    for keyword, pool in image_pools.items():
        if keyword in query_lower:
            selected_pool = keyword
            break
    
    # ä»é€‰ä¸­çš„æ± ä¸­éšæœºé€‰æ‹©å›¾ç‰‡
    pool_images = image_pools[selected_pool]
    selected_images = random.sample(pool_images, min(count, len(pool_images)))
    
    # å¦‚æœéœ€è¦çš„æ•°é‡è¶…è¿‡æ± ä¸­çš„å›¾ç‰‡ï¼Œé‡å¤é€‰æ‹©
    while len(selected_images) < count:
        selected_images.extend(random.sample(pool_images, min(count - len(selected_images), len(pool_images))))
    
    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    image_results = []
    for i, src in enumerate(selected_images[:count]):
        image_info = {
            "id": random.randint(100000, 999999) + i,
            "src": src,
            "width": 1920,
            "height": 1080,
            "alt": f"{query} image {i+1}",
            "photographer": "Pexels",
            "url": src
        }
        image_results.append(image_info)
    
    return image_results


async def DocumentSearch(
    keyword: str, number: int,
    tool_context: ToolContext,
):
    """
    æ ¹æ®å…³é”®è¯æœç´¢æ–‡æ¡£
    :param keyword: str, æœç´¢çš„ç›¸å…³æ–‡æ¡£çš„å…³é”®è¯
    :return: è¿”å›æ¯ç¯‡æ–‡æ¡£æ•°æ®
    """
    agent_name = tool_context.agent_name
    print(f"Agent{agent_name}æ­£åœ¨è°ƒç”¨å·¥å…·ï¼šDocumentSearch: " + keyword)
    metadata = tool_context.state.get("metadata", {})
    if metadata is None:
        metadata = {}
    print(f"è°ƒç”¨å·¥å…·ï¼šDocumentSearchæ—¶ä¼ å…¥çš„metadata: {metadata}")
    print("æ–‡æ¡£æ£€ç´¢: " + keyword)
    start_time = time.time()
    results = sogou_weixin_search(keyword)
    if not results:
        return f"æ²¡æœ‰æœç´¢åˆ°{keyword}ç›¸å…³çš„æ–‡ç« "
    articles = []
    results = results[:number]
    for every_result in results:
        sougou_link = every_result["link"]
        real_url = get_real_url(sougou_link)
        # refererï¼šè¯·æ±‚æ¥æº
        content = get_article_content(real_url, referer=sougou_link)
        article = {
            "title": every_result["title"],
            "publish_time": every_result["publish_time"],
            "real_url": real_url,
            "content": content
        }
        articles.append(article)
    end_time = time.time()
    print(f"å…³é”®è¯{keyword}ç›¸å…³çš„æ–‡ç« å·²ç»è·å–å®Œæ¯•ï¼Œè·å–åˆ°{len(articles)}ç¯‡, è€—æ—¶{end_time - start_time}ç§’")
    metadata["tool_document_ids"] = articles
    tool_context.state["metadata"] = metadata
    return articles

def KnowledgeBaseSearch(keyword: str, tool_context: ToolContext):
    """
    æ ¹æ®å…³é”®è¯æœç´¢æ–‡æ¡£åº“
    :param keyword: str, æœç´¢çš„ç›¸å…³æ–‡æ¡£çš„å…³é”®è¯
    :return: è¿”å›æ¯ç¯‡æ–‡æ¡£æ•°æ®
    """
    topk = 5  # æœç´¢å‰5æ¡ç»“æœ
    metadata = tool_context.state.get("metadata", {})
    # å°±æ˜¯å¯¹åº”ç”¨æˆ·ä¸Šä¼ PDFæ–‡ä»¶
    user_id = metadata.get("user_id", 999)
    logger.info(f"â¤ï¸â¤ï¸â¤ï¸â¤ï¸ğŸ˜œğŸ˜œğŸ˜œğŸ˜œğŸ˜œè°ƒç”¨çŸ¥è¯†åº“æœç´¢æ¥å£, user_id: {user_id}, query: {keyword}, topk: {topk}")
    print(f"â¤ï¸â¤ï¸â¤ï¸â¤ï¸ğŸ˜œğŸ˜œğŸ˜œğŸ˜œğŸ˜œè°ƒç”¨çŸ¥è¯†åº“æœç´¢æ¥å£, user_id: {user_id}, query: {keyword}, topk: {topk}")
    PERSONAL_DB = os.environ.get('PERSONAL_DB', '')
    assert PERSONAL_DB, "PERSONAL_DB is not set"
    url = f"{PERSONAL_DB}/search"
    # æ­£ç¡®çš„è¯·æ±‚æ•°æ®æ ¼å¼
    data = {
        "userId": user_id,
        "query": keyword,
        "keyword": "",  # å…³é”®è¯åŒ¹é…ï¼Œæ˜¯å¦éœ€è¦å¼ºåˆ¶åŒ…å«ä¸€äº›å…³é”®è¯
        "topk": topk
    }
    headers = {'content-type': 'application/json'}
    try:
        # å‘é€POSTè¯·æ±‚
        response = httpx.post(url, json=data, headers=headers, timeout=20.0, trust_env=False)

        # æ£€æŸ¥HTTPçŠ¶æ€ç 
        response.raise_for_status()
        assert response.status_code == 200, f"{PERSONAL_DB}æœç´¢çŸ¥è¯†åº“æŠ¥é”™"

        # è§£æè¿”å›çš„JSONæ•°æ®
        result = response.json()
        documents = result.get("documents", [])
        metadatas = result.get("metadatas", [])
        data = {"documents": documents, "metadatas": metadatas}
        print("Response status:", response.status_code)
        print("Response body:", result)
        logger.info(f"{PERSONAL_DB}æœç´¢çŸ¥è¯†åº“è¿”å›çŠ¶æ€: {response.status_code}")
        logger.info(f"{PERSONAL_DB}æœç´¢çŸ¥è¯†åº“è¿”å›ç»“æœ: {result}")
        logger.info(f"{PERSONAL_DB}æœç´¢çŸ¥è¯†åº“æˆåŠŸ, è¿”å›ç»“æœ: {data}")
        return True, data
    except Exception as e:
        print(f"{PERSONAL_DB}æœç´¢çŸ¥è¯†åº“æŠ¥é”™: {e}")
        return False, f"{PERSONAL_DB}æœç´¢çŸ¥è¯†åº“æŠ¥é”™: {str(e)}"

if __name__ == '__main__':
    import asyncio
    
    async def test():
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        result = await SearchImage("technology", 3)
        print(json.dumps(result, indent=2, ensure_ascii=False))
    
    asyncio.run(test())
